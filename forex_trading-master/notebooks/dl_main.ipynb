{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case** there is issue with reloading, run the following to delete compiled Python cache.\n",
    "\n",
    "This is **typically** not needed.\n",
    "Often, restarting the notebook works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ..; cd arguments; rm -rf __pycache__\n",
    "!cd ..; cd constants; rm -rf __pycache__\n",
    "!cd ..; cd data; rm -rf __pycache__\n",
    "!cd ..; cd logger; rm -rf __pycache__\n",
    "!cd ..; cd models; rm -rf __pycache__\n",
    "!cd ..; cd utils; rm -rf __pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrace Notebook for Forex Trading\n",
    "\n",
    "This is for testing only. For batched runs a main file is still required.\n",
    "\n",
    "Keep autoreload for faster Python module reloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import Python default packges first, then those requiring `pip`, finally modules in the code structure. Keep alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from arguments import *\n",
    "from constants import *\n",
    "from data import *\n",
    "import models as supported_models\n",
    "from training_tools import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "* Setup interface with Google Cloud bucket.\n",
    "* Pretend to have command line argument.\n",
    "* Prepare data\n",
    "* Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique instance for GCStorage has been created\n"
     ]
    }
   ],
   "source": [
    "storage = GCStorage.get_CloudFS(project_name=PROJECT_NAME,\n",
    "                                bucket_name=GC_BUCKET,\n",
    "                                credential_path=CREDENTIAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new experiment at 2020-05-07 00:43:21\n",
      "User: jingbo\n",
      "Host: deep3.stanford.edu\n",
      "{'misc_args': {'exp_name': 'jingbo_jupyter_5', 'log_level': 4, 'fast_debug': True, 'save_dir': 'experiments/2020-05-07/jingbo_jupyter_5', 'log_file': 'experiments/2020-05-07/jingbo_jupyter_5/run_log.txt'}, 'data_args': {'candle_interval': 30, 'num_candles': 20, 'num_iterval_ahead': 4, 'currency_pair': 'USDCAD', 'num_workers': 1}, 'train_args': {'device': 'cpu', 'disp_steps': 1, 'max_epochs': 100, 'batch_size': 256, 'learning_rate': 0.0001, 'weight_decay': 0.9, 'clipping_value': 1.0}, 'model_args': {'model_type': 'DummyModel', 'emb_size': 32, 'hidden_size': 64, 'num_layers': 3}}\n",
      "Step 0/False Texts\n",
      "\t[setup/command_line: /deep/group/packages/lilsnake3/envs/eeg/lib/python3.6/site-packages/ipykernel_launcher.py --exp_name=jupyter --device=cpu --num_candles=20 --num_workers=1 --fast_debug=True --candle_interval=30 --log_level=4]\n",
      "\t[setup/arguments: {   'data_args': {   'candle_interval': 30,\n",
      "                     'currency_pair': 'USDCAD',\n",
      "                     'num_candles': 20,\n",
      "                     'num_iterval_ahead': 4,\n",
      "                     'num_workers': 1},\n",
      "    'misc_args': {   'exp_name': 'jingbo_jupyter_5',\n",
      "                     'fast_debug': True,\n",
      "                     'log_file': 'experiments/2020-05-07/jingbo_jupyter_5/run_log.txt',\n",
      "                     'log_level': 4,\n",
      "                     'save_dir': 'experiments/2020-05-07/jingbo_jupyter_5'},\n",
      "    'model_args': {   'emb_size': 32,\n",
      "                      'hidden_size': 64,\n",
      "                      'model_type': 'DummyModel',\n",
      "                      'num_layers': 3},\n",
      "    'train_args': {   'batch_size': 256,\n",
      "                      'clipping_value': 1.0,\n",
      "                      'device': 'cpu',\n",
      "                      'disp_steps': 1,\n",
      "                      'learning_rate': 0.0001,\n",
      "                      'max_epochs': 100,\n",
      "                      'weight_decay': 0.9}}]\n"
     ]
    }
   ],
   "source": [
    "all_args = parse_from_string('--exp_name=jupyter --device=cpu --num_candles=20 --num_workers=1 '\n",
    "                             '--fast_debug=True --candle_interval=30 --log_level=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = all_args.misc_args.logger\n",
    "device = all_args.train_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique instance for DataAPI has been created\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = get_dataloaders(all_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_func = supported_models.__dict__[all_args.model_args.model_type]\n",
    "model = model_init_func(all_args.model_args) # TODO: JBY. Enable Dataparallel, if needed.\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = get_loss_func()# TODO (JBY): Replace with proper evaluator\n",
    "optimizer = Optimizer(all_args, model.parameters(), len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Deep Learning)\n",
    "\n",
    "At the moment we only have a validation set, a test set has not been created yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 1/23 [00:37<13:48, 37.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▊         | 2/23 [00:55<11:03, 31.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 3/23 [01:12<09:07, 27.39s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 4/23 [01:30<07:46, 24.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 5/23 [01:47<06:41, 22.32s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 6/23 [02:04<05:51, 20.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 7/23 [02:22<05:15, 19.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 8/23 [02:38<04:41, 18.80s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 9/23 [02:56<04:17, 18.39s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 10/23 [03:13<03:55, 18.12s/it]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in range(all_args.train_args.max_epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        time_series, ground_truth = batch\n",
    "\n",
    "        pred = model(time_series.float())\n",
    "        \n",
    "        loss = loss_func(pred, ground_truth.float())\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}/{all_args.train_args.max_epochs}')\n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
