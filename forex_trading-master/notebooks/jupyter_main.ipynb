{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case** there is issue with reloading, run the following to delete compiled Python cache.\n",
    "\n",
    "This is **typically** not needed.\n",
    "Often, restarting the notebook works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ..; cd arguments; rm -rf __pycache__\n",
    "!cd ..; cd constants; rm -rf __pycache__\n",
    "!cd ..; cd data; rm -rf __pycache__\n",
    "!cd ..; cd logger; rm -rf __pycache__\n",
    "!cd ..; cd models; rm -rf __pycache__\n",
    "!cd ..; cd utils; rm -rf __pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrace Notebook for Forex Trading\n",
    "\n",
    "This is for testing only. For batched runs a main file is still required.\n",
    "\n",
    "Keep autoreload for faster Python module reloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import Python default packges first, then those requiring `pip`, finally modules in the code structure. Keep alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from arguments import *\n",
    "from constants import *\n",
    "from data import *\n",
    "import models as supported_models\n",
    "from training_tools import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "* Setup interface with Google Cloud bucket.\n",
    "* Pretend to have command line argument.\n",
    "* Prepare data\n",
    "* Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique instance for GCStorage has been created\n"
     ]
    }
   ],
   "source": [
    "storage = GCStorage.get_CloudFS(project_name=PROJECT_NAME,\n",
    "                                bucket_name=GC_BUCKET,\n",
    "                                credential_path=CREDENTIAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new experiment at 2020-05-04 07:27:55\n",
      "User: jingboyang\n",
      "Host: pg-cpu-1\n",
      "{'misc_args': {'exp_name': 'jingboyang_jupyter_7', 'log_level': 0, 'fast_debug': True, 'save_dir': 'experiments/2020-05-04/jingboyang_jupyter_7', 'log_file': 'experiments/2020-05-04/jingboyang_jupyter_7/run_log.txt'}, 'data_args': {'candle_interval': 30, 'num_candles': 4, 'num_iterval_ahead': 4, 'currency_pair': 'USDCAD', 'num_workers': 1}, 'train_args': {'device': 'cpu', 'disp_steps': 1, 'max_epochs': 100, 'batch_size': 256, 'learning_rate': 0.0001, 'weight_decay': 0.9, 'clipping_value': 1.0}, 'model_args': {'model_type': 'DummyModel', 'emb_size': 32, 'hidden_size': 64, 'num_layers': 3}}\n",
      "Step 0/False Texts\n",
      "\t[setup/command_line: /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py --exp_name=jupyter --device=cpu --num_candles=4 --num_workers=1 --fast_debug=True --candle_interval=30]\n",
      "\t[setup/arguments: {   'data_args': {   'candle_interval': 30,\n",
      "                     'currency_pair': 'USDCAD',\n",
      "                     'num_candles': 4,\n",
      "                     'num_iterval_ahead': 4,\n",
      "                     'num_workers': 1},\n",
      "    'misc_args': {   'exp_name': 'jingboyang_jupyter_7',\n",
      "                     'fast_debug': True,\n",
      "                     'log_file': 'experiments/2020-05-04/jingboyang_jupyter_7/run_log.txt',\n",
      "                     'log_level': 0,\n",
      "                     'save_dir': 'experiments/2020-05-04/jingboyang_jupyter_7'},\n",
      "    'model_args': {   'emb_size': 32,\n",
      "                      'hidden_size': 64,\n",
      "                      'model_type': 'DummyModel',\n",
      "                      'num_layers': 3},\n",
      "    'train_args': {   'batch_size': 256,\n",
      "                      'clipping_value': 1.0,\n",
      "                      'device': 'cpu',\n",
      "                      'disp_steps': 1,\n",
      "                      'learning_rate': 0.0001,\n",
      "                      'max_epochs': 100,\n",
      "                      'weight_decay': 0.9}}]\n"
     ]
    }
   ],
   "source": [
    "all_args = parse_from_string('--exp_name=jupyter --device=cpu --num_candles=4 --num_workers=1 --fast_debug=True --candle_interval=30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = all_args.misc_args.logger\n",
    "device = all_args.train_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_dataloaders(all_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_func = supported_models.__dict__[all_args.model_args.model_type]\n",
    "model = model_init_func(all_args.model_args) # TODO: JBY. Enable Dataparallel, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = get_loss()# TODO (JBY): Replace with proper evaluator\n",
    "optimizer = Optimizer(all_args, model.parameters(), len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Deep Learning)\n",
    "\n",
    "At the moment we only have a validation set, a test set has not been created yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(all_args.train_args.max_epochs):\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        time_series, ground_truth = batch\n",
    "\n",
    "        pred = model(input_seq)\n",
    "        \n",
    "        loss = loss_func(output, target_seq.view(-1).long())\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}/{all_args.train_args.max_epochs}')\n",
    "    print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Machine Learning)\n",
    "\n",
    "Here we only use the tools to gather data :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index[11690] (2019-02-08 01:25:00, 2019-02-08 01:27:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f57d8e5e9e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index[12378] (2019-02-08 07:09:00, 2019-02-08 07:11:00)\n",
      "Index[5987] (2019-02-06 01:53:30, 2019-02-06 01:55:30)\n",
      "Index[3946] (2019-02-05 08:53:00, 2019-02-05 08:55:00)\n",
      "Index[12862] (2019-02-08 11:11:00, 2019-02-08 11:13:00)\n",
      "Index[62] (2019-02-04 00:31:00, 2019-02-04 00:33:00)\n",
      "Index[13621] (2019-02-08 17:30:30, 2019-02-08 17:32:30)\n",
      "Index[1356] (2019-02-04 11:18:00, 2019-02-04 11:20:00)\n",
      "LP 4 has insufficient number of candles 1\n",
      "Index[4604] (2019-02-05 14:22:00, 2019-02-05 14:24:00)\n",
      "Index[9711] (2019-02-07 08:55:30, 2019-02-07 08:57:30)\n",
      "Index[758] (2019-02-04 06:19:00, 2019-02-04 06:21:00)\n",
      "Index[6978] (2019-02-06 10:09:00, 2019-02-06 10:11:00)\n",
      "Index[5436] (2019-02-05 21:18:00, 2019-02-05 21:20:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[3858] (2019-02-05 08:09:00, 2019-02-05 08:11:00)\n",
      "Index[3695] (2019-02-05 06:47:30, 2019-02-05 06:49:30)\n",
      "LP 1 has insufficient points\n",
      "Index[10142] (2019-02-07 12:31:00, 2019-02-07 12:33:00)\n",
      "Index[2328] (2019-02-04 19:24:00, 2019-02-04 19:26:00)\n",
      "Index[3520] (2019-02-05 05:20:00, 2019-02-05 05:22:00)\n",
      "Index[5731] (2019-02-05 23:45:30, 2019-02-05 23:47:30)\n",
      "LP 2 has insufficient number of candles 1\n",
      "Index[13554] (2019-02-08 16:57:00, 2019-02-08 16:59:00)\n",
      "Index[4618] (2019-02-05 14:29:00, 2019-02-05 14:31:00)\n",
      "Index[14048] (2019-02-08 21:04:00, 2019-02-08 21:06:00)\n",
      "LP 2 has insufficient number of candles 1\n",
      "Index[10251] (2019-02-07 13:25:30, 2019-02-07 13:27:30)\n",
      "Index[8047] (2019-02-06 19:03:30, 2019-02-06 19:05:30)\n",
      "LP 1 has insufficient number of candles 0\n",
      "Index[600] (2019-02-04 05:00:00, 2019-02-04 05:02:00)\n",
      "Index[4317] (2019-02-05 11:58:30, 2019-02-05 12:00:30)\n",
      "Index[5040] (2019-02-05 18:00:00, 2019-02-05 18:02:00)\n",
      "Index[7968] (2019-02-06 18:24:00, 2019-02-06 18:26:00)\n",
      "Index[1397] (2019-02-04 11:38:30, 2019-02-04 11:40:30)\n",
      "Index[14144] (2019-02-08 21:52:00, 2019-02-08 21:54:00)\n",
      "Index[3099] (2019-02-05 01:49:30, 2019-02-05 01:51:30)\n",
      "Index[3301] (2019-02-05 03:30:30, 2019-02-05 03:32:30)\n",
      "Index[2894] (2019-02-05 00:07:00, 2019-02-05 00:09:00)\n",
      "Index[10745] (2019-02-07 17:32:30, 2019-02-07 17:34:30)\n",
      "Index[2138] (2019-02-04 17:49:00, 2019-02-04 17:51:00)\n",
      "Index[6475] (2019-02-06 05:57:30, 2019-02-06 05:59:30)\n",
      "Index[8653] (2019-02-07 00:06:30, 2019-02-07 00:08:30)\n",
      "Index[6279] (2019-02-06 04:19:30, 2019-02-06 04:21:30)\n",
      "LP 1 has insufficient points\n",
      "Index[12091] (2019-02-08 04:45:30, 2019-02-08 04:47:30)\n",
      "Index[9716] (2019-02-07 08:58:00, 2019-02-07 09:00:00)\n",
      "Index[4249] (2019-02-05 11:24:30, 2019-02-05 11:26:30)\n",
      "Index[5878] (2019-02-06 00:59:00, 2019-02-06 01:01:00)\n",
      "Day end index exceed limit\n",
      "Index[13933] (2019-02-08 20:06:30, 2019-02-08 20:08:30)\n",
      "Index[5108] (2019-02-05 18:34:00, 2019-02-05 18:36:00)\n",
      "Index[12819] (2019-02-08 10:49:30, 2019-02-08 10:51:30)\n",
      "Index[329] (2019-02-04 02:44:30, 2019-02-04 02:46:30)\n",
      "Index[4327] (2019-02-05 12:03:30, 2019-02-05 12:05:30)\n",
      "Index[5525] (2019-02-05 22:02:30, 2019-02-05 22:04:30)\n",
      "LP 1 has insufficient points\n",
      "Index[2189] (2019-02-04 18:14:30, 2019-02-04 18:16:30)\n",
      "Index[5983] (2019-02-06 01:51:30, 2019-02-06 01:53:30)\n",
      "Index[5127] (2019-02-05 18:43:30, 2019-02-05 18:45:30)\n",
      "Index[8480] (2019-02-06 22:40:00, 2019-02-06 22:42:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[6151] (2019-02-06 03:15:30, 2019-02-06 03:17:30)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[12646] (2019-02-08 09:23:00, 2019-02-08 09:25:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[5713] (2019-02-05 23:36:30, 2019-02-05 23:38:30)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[9401] (2019-02-07 06:20:30, 2019-02-07 06:22:30)\n",
      "Index[5937] (2019-02-06 01:28:30, 2019-02-06 01:30:30)\n",
      "Index[12899] (2019-02-08 11:29:30, 2019-02-08 11:31:30)\n",
      "Index[10796] (2019-02-07 17:58:00, 2019-02-07 18:00:00)\n",
      "Index[6774] (2019-02-06 08:27:00, 2019-02-06 08:29:00)\n",
      "Index[1031] (2019-02-04 08:35:30, 2019-02-04 08:37:30)\n",
      "Index[948] (2019-02-04 07:54:00, 2019-02-04 07:56:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[2816] (2019-02-04 23:28:00, 2019-02-04 23:30:00)\n",
      "Index[2128] (2019-02-04 17:44:00, 2019-02-04 17:46:00)\n",
      "Index[7676] (2019-02-06 15:58:00, 2019-02-06 16:00:00)\n",
      "Index[5871] (2019-02-06 00:55:30, 2019-02-06 00:57:30)\n",
      "Index[6880] (2019-02-06 09:20:00, 2019-02-06 09:22:00)\n",
      "Index[3787] (2019-02-05 07:33:30, 2019-02-05 07:35:30)\n",
      "Index[12402] (2019-02-08 07:21:00, 2019-02-08 07:23:00)\n",
      "Index[800] (2019-02-04 06:40:00, 2019-02-04 06:42:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[13578] (2019-02-08 17:09:00, 2019-02-08 17:11:00)\n",
      "Index[11281] (2019-02-07 22:00:30, 2019-02-07 22:02:30)\n",
      "Index[4472] (2019-02-05 13:16:00, 2019-02-05 13:18:00)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../data/data_manager.py\", line 90, in __getitem__\n    get_df=False)\n  File \"../utils/format_conversion.py\", line 116, in generate_candles\n    assert start_i is not None\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0a8f2fa7bc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtime_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../data/data_manager.py\", line 90, in __getitem__\n    get_df=False)\n  File \"../utils/format_conversion.py\", line 116, in generate_candles\n    assert start_i is not None\nAssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index[1159] (2019-02-04 09:39:30, 2019-02-04 09:41:30)\n",
      "Index[5671] (2019-02-05 23:15:30, 2019-02-05 23:17:30)\n",
      "LP 4 has insufficient points\n",
      "Index[950] (2019-02-04 07:55:00, 2019-02-04 07:57:00)\n",
      "Index[8692] (2019-02-07 00:26:00, 2019-02-07 00:28:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[8267] (2019-02-06 20:53:30, 2019-02-06 20:55:30)\n",
      "Index[12056] (2019-02-08 04:28:00, 2019-02-08 04:30:00)\n",
      "Index[10793] (2019-02-07 17:56:30, 2019-02-07 17:58:30)\n",
      "Index[6595] (2019-02-06 06:57:30, 2019-02-06 06:59:30)\n",
      "Index[6895] (2019-02-06 09:27:30, 2019-02-06 09:29:30)\n",
      "Index[7065] (2019-02-06 10:52:30, 2019-02-06 10:54:30)\n",
      "LP 1 has insufficient number of candles 0\n",
      "Index[3694] (2019-02-05 06:47:00, 2019-02-05 06:49:00)\n",
      "LP 1 has insufficient points\n",
      "Index[13540] (2019-02-08 16:50:00, 2019-02-08 16:52:00)\n",
      "Index[1347] (2019-02-04 11:13:30, 2019-02-04 11:15:30)\n",
      "Index[7199] (2019-02-06 11:59:30, 2019-02-06 12:01:30)\n",
      "Index[13158] (2019-02-08 13:39:00, 2019-02-08 13:41:00)\n",
      "Index[1207] (2019-02-04 10:03:30, 2019-02-04 10:05:30)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[6329] (2019-02-06 04:44:30, 2019-02-06 04:46:30)\n",
      "Index[2262] (2019-02-04 18:51:00, 2019-02-04 18:53:00)\n",
      "Index[2437] (2019-02-04 20:18:30, 2019-02-04 20:20:30)\n",
      "Index[6733] (2019-02-06 08:06:30, 2019-02-06 08:08:30)\n",
      "Index[13027] (2019-02-08 12:33:30, 2019-02-08 12:35:30)\n",
      "Index[4202] (2019-02-05 11:01:00, 2019-02-05 11:03:00)\n",
      "LP 1 has insufficient number of candles 0\n",
      "Index[12149] (2019-02-08 05:14:30, 2019-02-08 05:16:30)\n",
      "Index[9927] (2019-02-07 10:43:30, 2019-02-07 10:45:30)\n",
      "Index[13524] (2019-02-08 16:42:00, 2019-02-08 16:44:00)\n",
      "Index[9115] (2019-02-07 03:57:30, 2019-02-07 03:59:30)\n",
      "Index[4806] (2019-02-05 16:03:00, 2019-02-05 16:05:00)\n",
      "Index[10444] (2019-02-07 15:02:00, 2019-02-07 15:04:00)\n",
      "Index[10043] (2019-02-07 11:41:30, 2019-02-07 11:43:30)\n",
      "Index[2752] (2019-02-04 22:56:00, 2019-02-04 22:58:00)\n",
      "LP 2 has insufficient points\n",
      "Index[9581] (2019-02-07 07:50:30, 2019-02-07 07:52:30)\n",
      "Index[4244] (2019-02-05 11:22:00, 2019-02-05 11:24:00)\n",
      "LP 1 has insufficient points\n",
      "Index[12248] (2019-02-08 06:04:00, 2019-02-08 06:06:00)\n",
      "Index[631] (2019-02-04 05:15:30, 2019-02-04 05:17:30)\n",
      "Index[11985] (2019-02-08 03:52:30, 2019-02-08 03:54:30)\n",
      "Index[2525] (2019-02-04 21:02:30, 2019-02-04 21:04:30)\n",
      "Index[10276] (2019-02-07 13:38:00, 2019-02-07 13:40:00)\n",
      "Index[2960] (2019-02-05 00:40:00, 2019-02-05 00:42:00)\n",
      "Index[11855] (2019-02-08 02:47:30, 2019-02-08 02:49:30)\n",
      "Index[7798] (2019-02-06 16:59:00, 2019-02-06 17:01:00)\n",
      "Index[11190] (2019-02-07 21:15:00, 2019-02-07 21:17:00)\n",
      "LP 1 has insufficient points\n",
      "Index[4585] (2019-02-05 14:12:30, 2019-02-05 14:14:30)\n",
      "Index[3659] (2019-02-05 06:29:30, 2019-02-05 06:31:30)\n",
      "Index[8539] (2019-02-06 23:09:30, 2019-02-06 23:11:30)\n",
      "LP 4 has insufficient points\n",
      "Index[2559] (2019-02-04 21:19:30, 2019-02-04 21:21:30)\n",
      "LP 2 has insufficient points\n",
      "Index[11430] (2019-02-07 23:15:00, 2019-02-07 23:17:00)\n",
      "Index[13418] (2019-02-08 15:49:00, 2019-02-08 15:51:00)\n",
      "Index[9303] (2019-02-07 05:31:30, 2019-02-07 05:33:30)\n",
      "LP 2 has insufficient points\n",
      "Index[10254] (2019-02-07 13:27:00, 2019-02-07 13:29:00)\n",
      "Index[4775] (2019-02-05 15:47:30, 2019-02-05 15:49:30)\n",
      "Index[115] (2019-02-04 00:57:30, 2019-02-04 00:59:30)\n",
      "Index[13883] (2019-02-08 19:41:30, 2019-02-08 19:43:30)\n",
      "Index[5842] (2019-02-06 00:41:00, 2019-02-06 00:43:00)\n",
      "Index[5151] (2019-02-05 18:55:30, 2019-02-05 18:57:30)\n",
      "Index[6843] (2019-02-06 09:01:30, 2019-02-06 09:03:30)\n",
      "Index[5718] (2019-02-05 23:39:00, 2019-02-05 23:41:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[1135] (2019-02-04 09:27:30, 2019-02-04 09:29:30)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[4470] (2019-02-05 13:15:00, 2019-02-05 13:17:00)\n",
      "Index[10523] (2019-02-07 15:41:30, 2019-02-07 15:43:30)\n",
      "Index[13113] (2019-02-08 13:16:30, 2019-02-08 13:18:30)\n",
      "LP 4 has insufficient number of candles 1\n",
      "Index[6554] (2019-02-06 06:37:00, 2019-02-06 06:39:00)\n",
      "LP 1 has insufficient number of candles 0\n",
      "Index[4751] (2019-02-05 15:35:30, 2019-02-05 15:37:30)\n",
      "Index[805] (2019-02-04 06:42:30, 2019-02-04 06:44:30)\n",
      "Index[8497] (2019-02-06 22:48:30, 2019-02-06 22:50:30)\n",
      "LP 2 has insufficient points\n",
      "Index[3996] (2019-02-05 09:18:00, 2019-02-05 09:20:00)\n",
      "Index[11616] (2019-02-08 00:48:00, 2019-02-08 00:50:00)\n",
      "Index[408] (2019-02-04 03:24:00, 2019-02-04 03:26:00)\n",
      "Index[7078] (2019-02-06 10:59:00, 2019-02-06 11:01:00)\n",
      "Index[9105] (2019-02-07 03:52:30, 2019-02-07 03:54:30)\n",
      "Index[7844] (2019-02-06 17:22:00, 2019-02-06 17:24:00)\n",
      "Index[2229] (2019-02-04 18:34:30, 2019-02-04 18:36:30)\n",
      "Index[10810] (2019-02-07 18:05:00, 2019-02-07 18:07:00)\n",
      "Index[9417] (2019-02-07 06:28:30, 2019-02-07 06:30:30)\n",
      "Index[13723] (2019-02-08 18:21:30, 2019-02-08 18:23:30)\n",
      "Index[10818] (2019-02-07 18:09:00, 2019-02-07 18:11:00)\n",
      "Index[13465] (2019-02-08 16:12:30, 2019-02-08 16:14:30)\n",
      "Index[5532] (2019-02-05 22:06:00, 2019-02-05 22:08:00)\n",
      "LP 1 has insufficient points\n",
      "Index[13521] (2019-02-08 16:40:30, 2019-02-08 16:42:30)\n",
      "Index[6134] (2019-02-06 03:07:00, 2019-02-06 03:09:00)\n",
      "Index[4677] (2019-02-05 14:58:30, 2019-02-05 15:00:30)\n",
      "Index[13654] (2019-02-08 17:47:00, 2019-02-08 17:49:00)\n",
      "Index[2545] (2019-02-04 21:12:30, 2019-02-04 21:14:30)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[2670] (2019-02-04 22:15:00, 2019-02-04 22:17:00)\n",
      "LP 1 has insufficient points\n",
      "Index[6677] (2019-02-06 07:38:30, 2019-02-06 07:40:30)\n",
      "Index[11528] (2019-02-08 00:04:00, 2019-02-08 00:06:00)\n",
      "Index[8394] (2019-02-06 21:57:00, 2019-02-06 21:59:00)\n",
      "LP 1 has insufficient number of candles 1\n",
      "Index[8274] (2019-02-06 20:57:00, 2019-02-06 20:59:00)\n",
      "Index[9338] (2019-02-07 05:49:00, 2019-02-07 05:51:00)\n",
      "LP 4 has insufficient number of candles 1\n",
      "Index[5891] (2019-02-06 01:05:30, 2019-02-06 01:07:30)\n",
      "Index[6568] (2019-02-06 06:44:00, 2019-02-06 06:46:00)\n",
      "Index[3117] (2019-02-05 01:58:30, 2019-02-05 02:00:30)\n"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "for batch in tqdm(train_loader):    \n",
    "    time_series, ground_truth = batch\n",
    "    \n",
    "    print(time_series.shape)\n",
    "    print(ground_truth)\n",
    "    train_x.append(time_series.numpy().flatten())\n",
    "    train_y.append(ground_truth.numpy().flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
