{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from arguments import *\n",
    "from constants import *\n",
    "from data import *\n",
    "import models as supported_models\n",
    "from training_tools import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique instance for GCStorage has been created\n"
     ]
    }
   ],
   "source": [
    "storage = GCStorage.get_CloudFS(project_name=PROJECT_NAME,\n",
    "                                bucket_name=GC_BUCKET,\n",
    "                                credential_path=CREDENTIAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new experiment at 2020-06-02 00:49:34\n",
      "User: jingbo\n",
      "Host: pg-cpu-1\n",
      "{'misc_args': {'exp_name': 'jingbo_ml_3', 'exp_setting': 'simple_binary', 'log_level': 4, 'fast_debug': False, 'save_dir': 'experiments/2020-06-01/jingbo_ml_3', 'log_file': 'experiments/2020-06-01/jingbo_ml_3/run_log.txt'}, 'data_args': {'candle_interval': 60, 'num_candles': 16, 'num_iterval_ahead': 4, 'currency_pair': 'EURUSD', 'num_workers': 1}, 'train_args': {'loss_func': 'ce', 'device': 'cpu', 'disp_steps': 10, 'eval_steps': 99999, 'max_epochs': 100, 'batch_size': 256, 'learning_rate': 0.0001, 'weight_decay': 0.9, 'clipping_value': 1.0}, 'model_args': {'model_type': 'DummyModel', 'emb_size': 32, 'hidden_size': 64, 'num_layers': 3}}\n",
      "Step 0/False Texts\n",
      "\t[setup/command_line: /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py --exp_name=ml --device=cpu --currency_pair=EURUSD --num_candles=16 --candle_interval=60 --num_workers=1 --log_level=4 --exp_setting=simple_binary]\n",
      "\t[setup/arguments: {   'data_args': {   'candle_interval': 60,\n",
      "                     'currency_pair': 'EURUSD',\n",
      "                     'num_candles': 16,\n",
      "                     'num_iterval_ahead': 4,\n",
      "                     'num_workers': 1},\n",
      "    'misc_args': {   'exp_name': 'jingbo_ml_3',\n",
      "                     'exp_setting': 'simple_binary',\n",
      "                     'fast_debug': False,\n",
      "                     'log_file': 'experiments/2020-06-01/jingbo_ml_3/run_log.txt',\n",
      "                     'log_level': 4,\n",
      "                     'save_dir': 'experiments/2020-06-01/jingbo_ml_3'},\n",
      "    'model_args': {   'emb_size': 32,\n",
      "                      'hidden_size': 64,\n",
      "                      'model_type': 'DummyModel',\n",
      "                      'num_layers': 3},\n",
      "    'train_args': {   'batch_size': 256,\n",
      "                      'clipping_value': 1.0,\n",
      "                      'device': 'cpu',\n",
      "                      'disp_steps': 10,\n",
      "                      'eval_steps': 99999,\n",
      "                      'learning_rate': 0.0001,\n",
      "                      'loss_func': 'ce',\n",
      "                      'max_epochs': 100,\n",
      "                      'weight_decay': 0.9}}]\n",
      "Unique instance for CandleDataAPI has been created\n"
     ]
    }
   ],
   "source": [
    "all_args = parse_from_string('--exp_name=ml --device=cpu '\n",
    "                             '--currency_pair=EURUSD '\n",
    "                             '--num_candles=16 --candle_interval=60 '\n",
    "                             '--num_workers=1 '\n",
    "                             '--log_level=4 '\n",
    "                             '--exp_setting=simple_binary')\n",
    "\n",
    "logger = all_args.misc_args.logger\n",
    "train_loader, valid_loader = get_dataloaders(all_args)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [1]/19800\n",
      "# [501]/19800\n",
      "# [1001]/19800\n",
      "# [1501]/19800\n",
      "# [2001]/19800\n",
      "# [2501]/19800\n",
      "# [3001]/19800\n",
      "# [3501]/19800\n",
      "# [4001]/19800\n",
      "# [4501]/19800\n",
      "# [5001]/19800\n",
      "# [5501]/19800\n",
      "# [6001]/19800\n",
      "# [6501]/19800\n",
      "# [7001]/19800\n",
      "# [7501]/19800\n",
      "# [8001]/19800\n",
      "# [8501]/19800\n",
      "# [9001]/19800\n",
      "# [9501]/19800\n",
      "# [10001]/19800\n",
      "# [10501]/19800\n",
      "# [11001]/19800\n",
      "# [11501]/19800\n",
      "# [12001]/19800\n",
      "# [12501]/19800\n",
      "# [13001]/19800\n",
      "# [13501]/19800\n",
      "# [14001]/19800\n",
      "# [14501]/19800\n",
      "# [15001]/19800\n",
      "# [15501]/19800\n",
      "# [16001]/19800\n",
      "# [16501]/19800\n",
      "# [17001]/19800\n",
      "# [17501]/19800\n",
      "# [18001]/19800\n",
      "# [18501]/19800\n",
      "# [19001]/19800\n",
      "# [19501]/19800\n"
     ]
    }
   ],
   "source": [
    "def gether_data(loader):\n",
    "    all_x, all_gt = [], []\n",
    "    total_len = len(loader.dataset)\n",
    "    for i in range(total_len):\n",
    "        x, gt = loader.dataset[i]\n",
    "        if i % 500 == 0:\n",
    "            logger.log_stdout(f'# [{i + 1}]/{total_len}')\n",
    "\n",
    "        x = x.flatten()\n",
    "        # gt = gt\n",
    "\n",
    "        all_x.append(x)\n",
    "        all_gt.append(gt)\n",
    "    \n",
    "    return all_x, all_gt\n",
    "\n",
    "train_x, train_gt = gether_data(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "             estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
      "                                      colsample_bytree=1.0,\n",
      "                                      importance_type='split',\n",
      "                                      learning_rate=0.1, max_depth=-1,\n",
      "                                      min_child_samples=20,\n",
      "                                      min_child_weight=0.001,\n",
      "                                      min_split_gain=0.0, n_estimators=100,\n",
      "                                      n_jobs=-1, num_leaves=31, objective=None,\n",
      "                                      random_state=None, reg_alpha=0.0,\n",
      "                                      reg_lambd...\n",
      "                                      subsample=1.0, subsample_for_bin=200000,\n",
      "                                      subsample_freq=0),\n",
      "             iid='warn', n_jobs=28,\n",
      "             param_grid={'class_weight': ['balanced'], 'max_depth': [8],\n",
      "                         'min_child_samples': [0], 'min_split_gain': [0],\n",
      "                         'n_estimators': [128], 'num_leaves': [127],\n",
      "                         'reg_alpha': [0.01], 'reg_lambda': [0.7]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring=make_scorer(f1_score), verbose=10)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=28)]: Using backend LokyBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=28)]: Done   2 out of   5 | elapsed:   17.4s remaining:   26.1s\n",
      "[Parallel(n_jobs=28)]: Done   3 out of   5 | elapsed:   18.5s remaining:   12.3s\n",
      "[Parallel(n_jobs=28)]: Done   5 out of   5 | elapsed:   21.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=28)]: Done   5 out of   5 | elapsed:   21.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                      colsample_bytree=1.0,\n",
       "                                      importance_type='split',\n",
       "                                      learning_rate=0.1, max_depth=-1,\n",
       "                                      min_child_samples=20,\n",
       "                                      min_child_weight=0.001,\n",
       "                                      min_split_gain=0.0, n_estimators=100,\n",
       "                                      n_jobs=-1, num_leaves=31, objective=None,\n",
       "                                      random_state=None, reg_alpha=0.0,\n",
       "                                      reg_lambd...\n",
       "                                      subsample=1.0, subsample_for_bin=200000,\n",
       "                                      subsample_freq=0),\n",
       "             iid='warn', n_jobs=28,\n",
       "             param_grid={'class_weight': ['balanced'], 'max_depth': [8],\n",
       "                         'min_child_samples': [0], 'min_split_gain': [0],\n",
       "                         'n_estimators': [128], 'num_leaves': [127],\n",
       "                         'reg_alpha': [0.01], 'reg_lambda': [0.7]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint as pp\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "classifier_list = supported_models.classifiers\n",
    "metric_list = supported_models.binary_classification_metrics\n",
    "all_models = {model_name: classifier_list[model_name] \\\n",
    "                                                for model_name in classifier_list}\n",
    "\n",
    "model_name = 'lgb'\n",
    "model = all_models[model_name]\n",
    "\n",
    "# balanced \t8 \t0 \t0 \t128 \t127 \t0.01 \t0.7 \t{'class_weight': 'balanced', 'max_depth': 8, '... \t0.746114\n",
    "param_dict = {'min_child_samples': [0],\n",
    "               'class_weight':['balanced'],\n",
    "               'max_depth': [8],\n",
    "               'num_leaves': [127],\n",
    "               'min_split_gain': [0],\n",
    "               'reg_alpha': [0.01],\n",
    "               'reg_lambda': [0.7,],\n",
    "               'n_estimators': [128]}\n",
    "\n",
    "scoring_fn = make_scorer(\n",
    "            metric_list['f1'][0],\n",
    "            greater_is_better=metric_list['f1'][1])\n",
    "\n",
    "model = GridSearchCV(model, param_dict, scoring=scoring_fn, cv=5, refit=True, n_jobs=28, verbose=10)\n",
    "print(model)\n",
    "model.fit(train_x, train_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_days, valid_days = TRAIN_DAYS, VALID_DAYS\n",
    "\n",
    "# Cheat to get the dataset class\n",
    "dataset_class = globals()['SingleFXDatasetBase']\n",
    "\n",
    "valid_dataset = dataset_class(valid_days, all_args.misc_args.logger, all_args.data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.13389   1.13383   1.13379   1.13377   1.13377   1.1339    1.13395\n",
      "    1.13401   1.134     1.13418   1.13438   1.13432   1.13415   1.1341\n",
      "    1.13409   1.13409]\n",
      " [  1.13417   1.13389   1.13385   1.13381   1.13377   1.13392   1.13395\n",
      "    1.13403   1.13401   1.13418   1.13439   1.13441   1.13432   1.13419\n",
      "    1.13411   1.13409]\n",
      " [  1.13387   1.1338    1.13377   1.13377   1.13377   1.13377   1.1339\n",
      "    1.13392   1.134     1.13397   1.13415   1.13431   1.13414   1.1341\n",
      "    1.13409   1.13406]\n",
      " [  1.13417   1.13388   1.13382   1.13381   1.13377   1.13378   1.13391\n",
      "    1.13395   1.13401   1.13399   1.13417   1.13438   1.13431   1.13416\n",
      "    1.13411   1.13409]\n",
      " [167.       44.       87.       15.        0.       66.       38.\n",
      "   72.        6.       74.      180.       68.       58.       62.\n",
      "    9.       21.     ]\n",
      " [  1.13398   1.1339    1.13391   1.13384   1.13384   1.13398   1.13401\n",
      "    1.13408   1.13406   1.13425   1.13444   1.13437   1.13423   1.13417\n",
      "    1.13416   1.13416]\n",
      " [  1.13424   1.13398   1.13393   1.13391   1.13384   1.13402   1.13401\n",
      "    1.1341    1.13408   1.13425   1.13447   1.13448   1.13437   1.13426\n",
      "    1.13418   1.13416]\n",
      " [  1.13398   1.1339    1.13387   1.13384   1.13384   1.13384   1.13398\n",
      "    1.13401   1.13406   1.13406   1.13423   1.13436   1.13422   1.13417\n",
      "    1.13416   1.13413]\n",
      " [  1.13424   1.13397   1.1339    1.13389   1.13384   1.13385   1.13399\n",
      "    1.13402   1.13408   1.13406   1.13424   1.13444   1.13436   1.13424\n",
      "    1.13418   1.13416]\n",
      " [167.       44.       87.       15.        0.       66.       38.\n",
      "   72.        6.       74.      180.       68.       58.       62.\n",
      "    9.       21.     ]]\n",
      "[(1.1340500000000002, 1.13409, 1.1340500000000002, 1.1341200000000002, 1.1341599999999998, 1.1341200000000002)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(valid_dataset)):\n",
    "    sample = valid_dataset.getitem(i)\n",
    "    print(sample[0])\n",
    "    print(sample[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
