{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In case** there is issue with reloading, run the following to delete compiled Python cache.\n",
    "\n",
    "This is **typically** not needed.\n",
    "Often, restarting the notebook works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ..; cd arguments; rm -rf __pycache__\n",
    "!cd ..; cd constants; rm -rf __pycache__\n",
    "!cd ..; cd data; rm -rf __pycache__\n",
    "!cd ..; cd logger; rm -rf __pycache__\n",
    "!cd ..; cd models; rm -rf __pycache__\n",
    "!cd ..; cd utils; rm -rf __pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gcloud if not already there\n",
    "!pip install lightgbm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root = os.environ['HOME'] + '/temp_store/candle_60_data/AUDUSD'\n",
    "days = sorted(os.listdir(root))\n",
    "for day in days:\n",
    "    print(day)\n",
    "    lps = sorted(os.listdir(root + '/' + day))\n",
    "    if len(lps) != 5:\n",
    "        print('  MISSING LP!')\n",
    "    for f in sorted(os.listdir(root + '/' + day)):\n",
    "        print(f'\\t{f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrance Notebook for Forex Trading\n",
    "\n",
    "This is for testing only. For batched runs a main file is still required.\n",
    "\n",
    "Keep autoreload for faster Python module reloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import Python default packges first, then those requiring `pip`, finally modules in the code structure. Keep alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from arguments import *\n",
    "from constants import *\n",
    "from data import *\n",
    "import models as supported_models\n",
    "from training_tools import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "* Setup interface with Google Cloud bucket.\n",
    "* Pretend to have command line argument.\n",
    "* Prepare data\n",
    "* Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gether_data(loader):\n",
    "    all_x, all_gt = [], []\n",
    "    total_len = len(loader.dataset)\n",
    "    for i in range(total_len):\n",
    "        x, gt = loader.dataset[i]\n",
    "        if i % 500 == 0:\n",
    "            logger.log_stdout(f'# [{i + 1}]/{total_len}')\n",
    "\n",
    "        x = x.flatten()\n",
    "        # gt = gt\n",
    "\n",
    "        all_x.append(x)\n",
    "        all_gt.append(gt)\n",
    "    \n",
    "    return all_x, all_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = GCStorage.get_CloudFS(project_name=PROJECT_NAME,\n",
    "                                bucket_name=GC_BUCKET,\n",
    "                                credential_path=CREDENTIAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = parse_from_string('--exp_name=ml --device=cpu '\n",
    "                             '--currency_pair=EURUSD '\n",
    "                             '--num_candles=4 --candle_interval=300 '\n",
    "                             '--num_workers=1 '\n",
    "                             '--log_level=4 '\n",
    "                             '--exp_setting=simple_regression')\n",
    "\n",
    "logger = all_args.misc_args.logger\n",
    "train_loader, valid_loader = get_dataloaders(all_args)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_gt = gether_data(train_loader)\n",
    "valid_x, valid_gt = gether_data(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Machine Learning) (Tweak One Model with GridSearchCV)\n",
    "\n",
    "Here we only use the tools to gather data :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALL_SETTINGS[all_args.misc_args.exp_setting]['type'] == 'regression':\n",
    "    regressors_list = supported_models.regressors\n",
    "    metric_list = supported_models.regression_metrics\n",
    "    all_models = {model_name: regressors_list[model_name] \\\n",
    "                                                for model_name in regressors_list}\n",
    "elif ALL_SETTINGS[all_args.misc_args.exp_setting]['type'] == 'binary_classification':\n",
    "    classifier_list = supported_models.classifiers\n",
    "    metric_list = supported_models.binary_classification_metrics\n",
    "    all_models = {model_name: classifier_list[model_name] \\\n",
    "                                                for model_name in classifier_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kernel_ridge'\n",
    "model = all_models[model_name]\n",
    "\n",
    "'''\n",
    "param_dict = {'min_child_samples': [0, 2],\n",
    "               'class_weight':['balanced'],\n",
    "               'max_depth': [4, 6, 8],\n",
    "               'num_leaves': [31, 63, 127],\n",
    "               'min_split_gain': [0],\n",
    "               'reg_alpha': [0.01, 0.05],\n",
    "               'reg_lambda': [0.6, 0.7,],\n",
    "               'n_estimators': [64, 128]}\n",
    "'''\n",
    "param_dict = {'alpha': [0.01, 0.1, 1], 'kernel': ['linear']}\n",
    "# param_dict = {}\n",
    "\n",
    "scoring_fn = make_scorer(\n",
    "            metric_list['kendaltau'][0],\n",
    "            greater_is_better=metric_list['kendaltau'][1])\n",
    "\n",
    "model = GridSearchCV(model, param_dict, scoring=scoring_fn, cv=5, refit=True, n_jobs=28, verbose=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_x, train_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_df = pd.DataFrame(model.cv_results_)\n",
    "fit_df[[c for c in fit_df.columns if 'param' in c or 'mean_test_score' in c]].sort_values(['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confidence_interval(ground_truth, predictions,\n",
    "                                 score_func,\n",
    "                                 num_trials=500,\n",
    "                                 confidence_level=0.95):\n",
    "    '''Generate confidence interval given ground truth and prediction.\n",
    "    \n",
    "    Bootstrapping to get confidence interval for prediction.\n",
    "\n",
    "    Argument:\n",
    "        data (dict): ground truth and predictions\n",
    "        num_trials (int): number of trials for bootstrapping\n",
    "        confidence_level (float): confidence level\n",
    "    \n",
    "    Return:\n",
    "        dict({lb: float, mean: float, ub: float}): confidence interval\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ground_truth = np.array(ground_truth)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    scores = []\n",
    "    num_successes = 0\n",
    "    num_tries = 0\n",
    "    indices = list(range(len(ground_truth)))\n",
    "\n",
    "    while (num_successes < num_trials):\n",
    "        # Limit the number of tries.\n",
    "        num_tries += 1\n",
    "        if num_tries > 2 * num_trials:\n",
    "            raise ValueError(\n",
    "                \"Too many unsuccessful tries to compute metric.\")\n",
    "\n",
    "        # Handle case where only one class is included by indices.\n",
    "        new_indices = np.random.choice(indices, size=len(indices))\n",
    "        score = score_func(ground_truth[new_indices],\n",
    "                             predictions[new_indices])\n",
    "        scores.append(score)\n",
    "        num_successes += 1\n",
    "\n",
    "    mean = np.mean(scores)\n",
    "    scores.sort()\n",
    "    # Computed using basic bootstrap\n",
    "    lower = 2 * mean - scores[\n",
    "        int(((1 + confidence_level) / 2) * num_successes)]\n",
    "    upper = 2 * mean - scores[\n",
    "        int(((1 - confidence_level) / 2) * num_successes)]\n",
    "\n",
    "    result = {'lb': lower, 'mean': mean, 'ub': upper}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_x)\n",
    "valid_pred = model.predict(valid_x)\n",
    "\n",
    "tr = {}\n",
    "vr = {}\n",
    "\n",
    "vr_ci = {}\n",
    "for metric in metric_list:\n",
    "    train_score = metric_list[metric][0](train_gt, train_pred)\n",
    "    valid_score = metric_list[metric][0](valid_gt, valid_pred)\n",
    "    # print(f'{metric}: {valid_score}')\n",
    "\n",
    "    tr[metric] = train_score\n",
    "    vr[metric] = valid_score\n",
    "\n",
    "    ci = generate_confidence_interval(valid_gt, valid_pred,\n",
    "                                 metric_list[metric][0],\n",
    "                                 num_trials=500,\n",
    "                                 confidence_level=0.95)\n",
    "    \n",
    "    vr_ci[metric] = ci\n",
    "    \n",
    "    \n",
    "joint_df = pd.DataFrame({'Train': tr, 'Valid': vr})\n",
    "joint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ci_df = pd.DataFrame.from_dict(vr_ci, orient='index')\n",
    "valid_ci_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the Actual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(valid_gt, valid_pred, s=2)\n",
    "plt.hlines(1, 0.9975, 1.0025, linewidth=1)\n",
    "plt.vlines(1, 0.9975, 1.0025, linewidth=1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
